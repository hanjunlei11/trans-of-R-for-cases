# transformer-of-Recommendation-for-similar-cases
项目简介：

给定一篇裁判文书，在数据库中检索出若干篇与之相似度最高的裁判文书。

项目方案：

由于没有相似度直接标签，所以利用远程标签来训练网络。

首先，我们人为，案件本身的信息，可以用罪名，刑期，罚金，是否财产型犯罪，是否暴力犯罪等属性来代替

所以，我们用以上属性作为该案件的标签，用属性相似来代替文本相似，具体做法是：

使用以上属性作为模型需要预测的标签，假设预测准确率极高，那我们可以人为我们得到一个拥有优秀编码能力的神经网络，该网络能够以极高的准确率将文本映射成标签。

我们将每个文本经过网络编码成的向量作为该文本的“语义”表示，保存在数据库中，这样，每篇裁判文书都得到一个代表自己语义的向量。

我们可以用余弦相似度、点积、等等可以度量相似度的方法来间接度量两个文本的相似度。

当然，如果每篇文档检索的时候都要和数据库的每个向量都计算一次相似度的话，计算量无疑是巨大的，系统响应速度将会非常缓慢。

所以我们在计算相似度之前先进行一次属性过滤，将相同属性的文书筛选出来，再进行相似度计算，计算量将会大大减少。

最后，根据相似度大小，排序将裁判文书文本输出。

各个属性的平均准确率达到94%，所以，基于我们的假设，远程标签作用在语义向量上的效果极好。

编码器我们采用最近在googe手里大放光彩的transformer编码器，这里我们只用了编码结构，没有用解码结构，解码结构是机器翻译任务专用的结构。

在bert里，transformer展现了它在自然语言处理方面的强势之处，尤其是在长距离依赖上，性能远超之前的LSTM。

但是transformer也有一个比较大的缺点，比较难训练，相比于LSTM，transformer的结构的训练难度简直上升了一个数量级，收敛速度慢，而且必须要大语料库才能去训练，参数足够多才能得到好的效果。

我们采用和bert一样的训练方式预训练我们的模型，然后在下游任务上微调模型。

3、模型创新点

（1）通过多任务的形式，从各方面提取文本语义信息，其中，多任务共享编码器参数。

（2）使用transformer基本结构，六层transformer作为编码器，解决了长距离依赖问题。

（3）模型训练完成后，辅助任务部分被丢弃，保留编码器及其之前的模型，将数据库里每一篇文档都表示成一个语义向量，作为该文档的表述。

（4）当一篇新的犯罪事实描述到来，首先经过编码器编码成一个语义表达向量，然后与预筛选出的一部分数据库里的裁判文书向量计算cos距离，作为相似度，最后排序输出。

4、实验结果

  单个任务分类准确率94%以上

